#!/usr/bin/env python
import defopt
from pathlib import Path
import json
import os
import shutil
import subprocess
import itertools
import stat
import glob
import multiprocessing
import functools



def distribute_samples(n_samples, n_workers):
    n_local = n_samples // n_workers
    res = [n_local for i in range(n_workers)]
    n_remainder = n_samples % n_workers
    for i in range(n_remainder):
        res[i] += 1
    return res


def remove_executables_in_dir(directory):
    executable = stat.S_IEXEC | stat.S_IXGRP | stat.S_IXOTH
    for root, dirs, files in os.walk(directory):
        for filename in files:
            path = os.path.join(root, filename)
            st = os.stat(path)
            mode = st.st_mode
            if mode & executable:
                os.remove(path)

def prepare_run_dir(worker_id: int, config: Path):
    """
    Prepare the run directoryy
    :param worker_id: worker Id
    :param config: JSON configuration file 
    """

    pid = multiprocessing.current_process()

    # read the config file to extract the metadata
    with config.open('r') as f:
        meta = json.load(f)

    # convert the integer Id to a string, prepending up to 4 zeros
    # this gives us a maximum number of 10,000 workers
    worker_id_str = str(worker_id).zfill(4)
    worker_run_path = Path(meta['run_dir']) / Path(f'worker_{worker_id_str}')

    # copy the SWAT files to the worker run directory
    print(f"...[{pid}] copying data from {meta['project_dir']} to {worker_run_path}")
    shutil.copytree(meta['project_dir'], worker_run_path)

    # remove any executable file
    print(f"...[{pid}] removing any executable file under {worker_run_path}")
    remove_executables_in_dir(worker_run_path)

    # copy the param input file
    input_rds = worker_run_path / Path('input.rds')
    print(f"...[{pid}] copying parameter table {meta['sim']['input']} to {input_rds}")
    shutil.copyfile(meta['sim']['input'], input_rds)

    # copy the executable over to the run directory. SWATPlusR wants the executable to 
    # be in this directory
    swat_exe = worker_run_path / Path('swat')
    print(f"...[{pid}] copying {meta['swat_exec']} to {swat_exe}")
    shutil.copyfile(meta['swat_exec'], swat_exe)
    # make it executable (UNIX)
    swat_exe.chmod(0o0766)

    n_workers = meta['sim']['n_workers']
    var_name, var_file = meta['sim']['output']['var'].split('.')
    units = meta['sim']['output']['units']

    # extract the number of samples by running an R code, which reads the RDS input file
    result = subprocess.run(["Rscript", "get_num_rows.R", meta['sim']['input']],
                            capture_output=True, text=True)
    n_samples = int(result.stdout)

    # assign the samples to this processs
    n_local_samples = distribute_samples(n_samples, n_workers)
    indx_end_local = [e for e in itertools.accumulate(n_local_samples)]
    indx_beg_local = [indx_end_local[i] - n_local_samples[i] for i in range(len(n_local_samples))]

    # build the run.R script
    r_script = f"""
library(SWATplusR)
library(dplyr)

# start row
ibeg <- {indx_beg_local[worker_id] + 1}
# end row
iend <- {indx_end_local[worker_id]}

# get the params for this worker
param <- readRDS('input.rds')[ibeg:iend,]

# run the parameter scan
res <- run_swat2012(project_path = \'./\',
                output = list(q=define_output(file = \'{var_file}\',
                          variable = \'{var_name}\', unit={units})),
                parameter = param,
                n_thread = {meta['sim']['n_threads_per_worker']})

# save the results
saveRDS(res, file = \'./result.rds\')
"""
    # write the run.R script
    with open(worker_run_path / Path(f'run.R'), 'w') as f:
        f.write(r_script)



def clean(*, config: Path):
    """
    Clean the experiment
    :param config: JSON configuration file
    """
    # read the config file
    with config.open('r') as f:
        meta = json.load(f)

    
    run_dir = Path(meta['run_dir'])
    if run_dir.is_dir():
        print(f"...removing {run_dir}. This can take some time...")
        shutil.rmtree(run_dir)
    else:
        print(f"...{run_dir} does not exist. You're good to go.")



def prep(*, config: Path, num_procs: int=1):
    """
    Prepare
    :param config: JSON configuration file
    :param num_procs: number of parallel processes used for copying input files
    """
    # read the config file
    with config.open('r') as f:
        meta = json.load(f)

    # runs some checks
    if not "project_dir" in meta:
        raise RuntimeError("need to have a project directory")
    if not os.path.isdir(meta["project_dir"]):
        raise RuntimeError(f"file {meta['project_dir']} is not a directory")
    if not "run_dir" in meta:
        raise RuntimeError("need to have a run directory")

    n_workers = meta['sim']['n_workers']
    prepare_run_dir_one = functools.partial(prepare_run_dir, config=config)
    with multiprocessing.Pool(num_procs) as pool:
        pool.map(prepare_run_dir_one, range(n_workers))



def run(*, config: Path):
    """
    Create SLURM run script
    :param config: configuration file
    """
    # read the config file
    with config.open('r') as f:
        meta = json.load(f)

    # create slurm job
    nworkers = meta['sim']['n_workers']
    nthreads = meta['sim']['n_threads_per_worker']
    act = meta['scheduler']['slurm']['account']
    tim = meta['scheduler']['slurm']['time']
    mem = meta['scheduler']['slurm']['mem']
    run_dir = meta['run_dir']
    if not os.path.isabs(run_dir):
        # relative path, make it absolute
        run_dir = Path(os.getcwd()) / Path(run_dir)

    slurm_script = f"""#!/bin/bash -e
#SBATCH --job-name=swt-{nworkers}w-{nthreads}t
#SBATCH --account={act}       
#SBATCH --time={tim}
#SBATCH --mem={mem}
#SBATCH --array=0-{nworkers - 1}
#SBATCH --ntasks=1
#SBATCH --cpus-per-task={nthreads}
#SBATCH --hint=nomultithread

# on mahuika, need these modules
ml Python R intel

# prepending the worker_id with zeros
worker_id=$(python -c "import sys; n = sys.argv[1]; print(n.zfill(4))" ${{SLURM_ARRAY_TASK_ID}})

cd {run_dir}/worker_${{worker_id}}
Rscript run.R
"""
    fname = meta['run_dir'] / Path(f'run.sl')
    with open(fname, 'w') as f:
            f.write(slurm_script)

    print(f"Now execute\nsbatch {fname}")


def merge(*, config: Path):
    """
    Analyse the results
    :param config: configuration file
    """
    # read the config file
    with config.open('r') as f:
        meta = json.load(f)

    run_dir = meta['run_dir']
    res = subprocess.run(["Rscript", "merge_partial_results.R", run_dir],
                        capture_output=True, text=True)
    # check the output
    if not "SUCCESS" in res.stdout:
        print('stdout:')
        print(res.stdout)
        print('stderr:')
        print(res.stderr)
        raise RuntimeError(f"Failed to merge the RDS parameter files. Check that files {run_dir}/worker_*/result.rds exist")
    print(res.stdout)


def plot(*, config: Path):
    """
    Plot the results
    :param config: configuration file
    """
    # read the config file
    with config.open('r') as f:
        meta = json.load(f)

    run_dir = Path(meta['run_dir'])
    sim_file = run_dir / Path('simulation.rds')
    units_beg, units_end = meta['sim']['output']['units'].split(':')
    units_lst = [str(i) for i in range(int(units_beg), int(units_end) + 1)]
    cmds = ["Rscript", "plot_results.R",  str(sim_file)] + units_lst
    res = subprocess.run(cmds,
                        capture_output=True, text=True)
    # check the output
    if not "SUCCESS" in res.stdout:
        print('stdout:')
        print(res.stdout)
        print('stderr:')
        print(res.stderr)
        raise RuntimeError(f"Failed to plot the RDS simulation result. Check that file {run_dir}/simulation.rds exists")
    print(res.stdout)




if __name__ == '__main__':
    defopt.run([clean, prep, run, merge, plot])
